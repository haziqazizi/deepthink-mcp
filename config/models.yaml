# DeepThink MCP Model Configuration
version: "1.0"

# Model definitions
models:
  o3-pro:
    provider: openai
    model_name: "o3-pro"
    name: "OpenAI O3 Pro"
    enabled: true
    capabilities: ["reasoning", "coding", "analysis", "math", "function_calling"]
    cost_per_1k_tokens: 0.002  # placeholder pricing
    default_params:
      temperature: null  # O3 models don't use temperature
      max_tokens: 4000
      reasoning_effort: "high"
    rate_limit:
      requests_per_minute: 50
      requests_per_day: 1000
    timeout_ms: 60000

# Global settings
settings:
  # Default model when no specific model is requested
  default_model: "o3-pro"

  # Fallback model if primary model fails (none for now)
  fallback_model: null

  # Models available for auto-selection
  available_models:
    - "o3-pro"

  # Global limits and settings
  max_retries: 3
  timeout_ms: 30000
  cache_ttl: 300



  # Global rate limiting (applied per client)
  rate_limits:
    global_requests_per_minute: 200
    per_user_requests_per_minute: 50
    burst_limit: 10

  # Model selection preferences (kept for future expansion)
  selection_preferences:
    speed_threshold_words: 10
    reasoning_keywords:
      - "analyze"
      - "explain"
      - "solve"
      - "complex"
      - "difficult"
      - "reasoning"
    cost_optimize: true
    cost_weight: 0.3
    quality_speed_balance: 0.7

# Environment-specific overrides
environments:
  development:
    rate_limits:
      per_user_requests_per_minute: 20
    logging_level: "debug"

  production:
    rate_limits:
      per_user_requests_per_minute: 100
    logging_level: "info"

  testing:
    default_model: "o3-pro"
    fallback_model: null
    rate_limits:
      per_user_requests_per_minute: 10
